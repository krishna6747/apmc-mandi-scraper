[README.md](https://github.com/user-attachments/files/22194783/README.md)
# eNAM APMC Mandi Scraper Â· Endâ€‘toâ€‘End Pipeline

> Extracts India APMC mandi contact details from the official eNAM portal with an automated, repeatable Selenium pipeline. Cleaned data lands as CSV â€” perfect for analysis and open-data projects.

![Status](https://img.shields.io/badge/status-active-brightgreen)
![Python](https://img.shields.io/badge/python-3.9%2B-blue)
![License](https://img.shields.io/badge/license-MIT-lightgrey)
![Selenium](https://img.shields.io/badge/selenium-automated-green)
![PRs welcome](https://img.shields.io/badge/PRs-welcome-orange)

## âœ¨ What this does
- Navigates to the official eNAM APMC contact page
- Selects a **state â†’ district â†’ all mandis** (no artificial cap)
- Extracts contact information from **tables + page text**
- Cleans and de-duplicates records
- Saves a timestamped CSV like `enam_clean_data_{epoch}.csv`

> **Heads-up:** The full state scrape can take time. Use `--max_districts` to run quick tests.

## ğŸ§  How it works (pipeline)
```mermaid
flowchart LR
    A[Start] --> B[Load eNAM APMC page]
    B --> C[Select language: English]
    C --> D[Select State]
    D --> E[Loop Districts]
    E --> F[Loop Mandis in District]
    F --> G[Extract from tables + text]
    G --> H[Clean + de-duplicate]
    H --> I[Save CSV]
```

## ğŸ›  Tech
- **Selenium** (headless Chrome), **pandas**
- Tested as a **Jupyter notebook** and **CLI script**

## ğŸš€ Quickstart
```bash
# 1) Create & activate a virtualenv (recommended)
python -m venv .venv
# Windows
.venv\Scripts\activate
# macOS/Linux
source .venv/bin/activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Make sure Chrome/Chromedriver are available on PATH
#    (or install with your OS package manager)
#    On Linux, verify: which google-chrome || which chromium-browser

# 4) Run the scraper (example: Maharashtra)
python scripts/run_scraper.py --state "Maharashtra"
# For a quick smoke test
python scripts/run_scraper.py --state "Maharashtra" --max_districts 1
```

The script will save a cleaned CSV in the **current working directory** and print a quick summary.

## ğŸ“¦ Project structure
```
enam-apmc-mandi-scraper/
â”œâ”€ src/enam_scraper/scraper.py     # Selenium scraper (logic preserved from notebook)
â”œâ”€ scripts/run_scraper.py          # CLI wrapper
â”œâ”€ notebooks/mandi_address.ipynb   # Original notebook
â”œâ”€ data/
â”‚  â”œâ”€ raw/                         # (optional) raw dumps
â”‚  â””â”€ processed/                   # (optional) cleaned data
â”œâ”€ .github/workflows/ci.yml        # Lint + smoke test
â”œâ”€ requirements.txt
â”œâ”€ .gitignore
â”œâ”€ LICENSE
â””â”€ README.md
```

## ğŸ“‘ Output schema (CSV)
- `state` Â· `district` Â· `mandi_name` Â· `address` Â· `contact_details`

##  GitHub
- Add **topics**: `apmc`, `india`, `enam`, `mandi`, `web-scraping`, `selenium`, `data-pipeline`, `open-data`
- Pin the repo to your profile and add a clear **oneâ€‘line tagline**
- Upload a **social preview image** (Settings â†’ General â†’ Social preview)
- Open a few good **starter issues** (e.g., â€œAdd Dockerfileâ€, â€œPublish dataset to releaseâ€)
- Add a short **demo GIF** of the scraper running (optional)

## ğŸ§ª CI (optional but recommended)
This repo ships with a simple GitHub Actions workflow that runs lint checks and a **non-network** smoke test so your PRs stay green.

## ğŸ¤ Responsible use
Scrapes public contact details from the official portal with delays and reasonable navigation. Please respect the websiteâ€™s terms and avoid overwhelming their servers. This project is for educational and openâ€‘data purposes.

## ğŸ“œ License
MIT Â© 2025 Krishna Vishwakarma
